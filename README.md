# NLP Classification Project

```
python3 -m venv my_env
source my_env/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
jupyter notebook
```

This project demonstrates multiple approaches to text classification using a finance-focused,
Moody‚Äôs-style dataset. The goal is to compare four major NLP modeling families:

1. **LLM Prompt-Based Classification** (zero-shot / few-shot / RAG-style)
2. **Transformer Encoder Fine-Tuning** (BERT / DistilBERT)
3. **XGBoost Classification** (with TF-IDF or embeddings)
4. **Logistic Regression Baseline** (TF-IDF)

The project includes a small synthetic credit-risk dataset that imitates analyst-style
sentences used for rating decisions, credit opinions, and outlook analysis.

## üöÄ Features

### **1. Prompt-Based LLM Classification (with RAG-Few-Shot)**
- Uses OpenAI `responses` API (`gpt-4.1-mini`)
- Retrieves similar labeled samples using TF-IDF cosine similarity
- Injects retrieved examples as few-shot demonstrations
- Zero training required

### **2. Transformer Encoder Fine-Tuning**
- Uses `AutoModelForSequenceClassification` (DistilBERT)
- PyTorch-based training loop
- GPU-friendly but can run CPU with smaller batch size
- Best supervised accuracy

### **3. XGBoost Classifier**
- Strong classical ML baseline
- Works well with TF-IDF
- Fast training and low latency
- Great for medium datasets

### **4. Logistic Regression Baseline**
- TF-IDF + LogisticRegression
- Very fast, interpretable
- Good first model to validate the dataset

## üìÅ Project Structure

```
nlp/
‚îÇ
‚îú‚îÄ‚îÄ data.py                     # Moody‚Äôs-style synthetic dataset
‚îÇ
‚îú‚îÄ‚îÄ llm_rag_few_shot.py    # LLM + RAG few-shot classifier
‚îú‚îÄ‚îÄ encoder_model.py            # DistilBERT fine-tuning
‚îú‚îÄ‚îÄ xgb_model.py                # XGBoost classifier
‚îú‚îÄ‚îÄ lr_model.py                 # Logistic Regression baseline
‚îÇ
‚îî‚îÄ‚îÄ utils/                      # (optional utilities)
```

## üìä Model Comparison

| Category | **LLM (Prompt / RAG)** | **Encoder (BERT / DistilBERT)** | **XGBoost (TF-IDF / Embeddings)** | **Logistic Regression (TF-IDF)** |
|---------|------------------------|----------------------------------|-----------------------------------|----------------------------------|
| **Training Required** | ‚ùå None | ‚úÖ Yes (fine-tuning) | ‚úÖ Yes | ‚úÖ Yes |
| **Data Needed** | ‚≠ê Very little (few-shot) | ‚≠ê‚≠ê Medium (1k‚Äì100k) | ‚≠ê Small‚ÄìMedium | ‚≠ê Small‚ÄìMedium |
| **Understands Context** | ‚≠ê‚≠ê‚≠ê‚≠ê Excellent (reasoning, credit nuance) | ‚≠ê‚≠ê‚≠ê‚≠ê Strong contextual | ‚≠ê‚≠ê Medium (bag-of-words) | ‚≠ê Weak (linear) |
| **Interpretability** | ‚≠ê Low | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê Medium | ‚≠ê‚≠ê‚≠ê‚≠ê High |
| **Latency** | ‚ùå Slow (API call) | Medium (GPU/CPU) | Fast | ‚ö° Very fast |
| **Compute Cost** | $$$ Highest | $$ Moderate | $ Low | $ Lowest |
| **Deployment Complexity** | Hard (API / LLM infra) | Medium | Easy | ‚≠ê Very easy |
| **Few-Shot Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê Needs training | ‚≠ê Medium | ‚≠ê Weak |
| **Large Dataset Performance** | Good but $$$ | ‚≠ê‚≠ê‚≠ê‚≠ê Best | ‚≠ê‚≠ê‚≠ê‚≠ê Strong | ‚≠ê‚≠ê Limited |
| **Captures Financial Nuance** | ‚≠ê‚≠ê‚≠ê‚≠ê Best | ‚≠ê‚≠ê‚≠ê Strong | ‚≠ê‚≠ê Medium | ‚≠ê Low |
| **Determinism** | Low‚ÄìMedium | High | High | Very High |
| **Best Use Cases** | Reasoning text, credit outlook, narrative risk | Rated text, credit sentiment, NER | Production classification, low-latency | Baseline model, sanity checks |

## üîß Setup

### 1. Install dependencies

```
mamba install -y scikit-learn
mamba install -y xgboost
mamba install -y transformers
mamba install -y torch
mamba install -y openai
mamba install -y spacy
python -m spacy download en_core_web_sm
```
